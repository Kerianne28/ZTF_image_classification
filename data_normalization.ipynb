{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing image data in preparation for classification:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data can often be manipulated in such a way that the underlying information isn't altered, but the data is better prepared for input into machine learning algorithms. Such manipulation is referred to as \"pre-processing\", and usually involves scaling (or \"standardizing\") the data, or applying the same operation to each data point. Pre-processing data usually results in better performance for machine learning algorithms.\n",
    "\n",
    "### This notebook:\n",
    "- Opens each cutout\n",
    "- Flattens each image, in preparation for the classifier\n",
    "- Applies 5 different data normalization techniques to each flattened image\n",
    "- Saves all flatted data to .csv, for correct input into MuyGPyS\n",
    "- Plots a few cutouts so you can visualize what each normalizing technique does to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import astropy.io.fits as fits\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting list of all cutout paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutout_path = 'cutouts/'\n",
    "cutout_files = []\n",
    "gals = 0\n",
    "stars = 0\n",
    "\n",
    "for file in os.listdir(cutout_path):\n",
    "    if file.endswith('.fits'):\n",
    "        cutout_files.append(file)\n",
    "        if file.startswith('gal'):\n",
    "            gals += 1\n",
    "        elif file.startswith('star'):\n",
    "            stars += 1\n",
    "\n",
    "print(\"Found\", len(cutout_files), \"cutouts:\", stars, \"stars, and\", gals, \"gals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.Random(128).shuffle(cutout_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten and save the un-normalized \"raw\" image data for every cutout. \n",
    "\n",
    "For each cutout:\n",
    "- Open the cutout\n",
    "- Set truth labels (0=Star, 1=Galaxy)\n",
    "- Flatten the data (turns 20x20 pixel image from shape (20, 20) to shape (400,))\n",
    "- Append the maximum and minimum pixel value (for use later)\n",
    "- Save to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DataFrame to append each cutout data to\n",
    "gal_star = pd.DataFrame({})\n",
    "\n",
    "# Define lists that each minimum and maximum pixel value will be appended to\n",
    "min_pixel = []\n",
    "max_pixel = []\n",
    "\n",
    "# Create a DataFrame to append plotting data to\n",
    "plot_data = pd.DataFrame({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, file in enumerate(tqdm(cutout_files)):\n",
    "    image = fits.getdata(cutout_path+str(file))\n",
    "    \n",
    "    if file.startswith('star')==True:\n",
    "        obj_id = 0\n",
    "    if file.startswith('galaxy')==True:\n",
    "        obj_id = 1\n",
    "         \n",
    "    data_flattened = image.flatten()\n",
    "    \n",
    "    min_pixel.append(np.min(data_flattened))\n",
    "    max_pixel.append(np.max(data_flattened))\n",
    "    \n",
    "    gal_star = gal_star.append([np.append(obj_id, data_flattened)], ignore_index=True)\n",
    "    \n",
    "plot_data = pd.concat([plot_data, pd.DataFrame({'raw': gal_star[:5].to_numpy().flatten()})], axis=1)\n",
    "gal_star.to_csv('raw_image_data.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the minimum and maximum pixel value across all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_pixel_all = np.min(min_pixel)\n",
    "max_pixel_all = np.max(max_pixel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define each data normalization technique function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technique 1: \n",
    "\n",
    "#### For each cutout:\n",
    "- Take the log of each pixel value\n",
    "- Find the minimum pixel value accross the image\n",
    "- Subtract that minimum value from each pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_1(data):\n",
    "    data_log = np.log10(data)\n",
    "    min_log_data = np.amin(data_log)\n",
    "    data_norm = data_log - min_log_data\n",
    "    return data_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technique 2:\n",
    "\n",
    "#### For each cutout:\n",
    "- Calculate minimum pixel value across image\n",
    "- Calculate maximum pixel value accross image\n",
    "- Scale all data between (0, 1) with:<br>\n",
    "$data\\_norm = \\frac{data - min}{max - min}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_2(data):\n",
    "    min_data = np.min(data)\n",
    "    max_data = np.max(data)\n",
    "    data_norm = (data - min_data) / (max_data - min_data)\n",
    "    return data_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technique 3:\n",
    "\n",
    "The same as technique 2, but now `min` and `max` are the minimum and maximum pixel value over ALL images\n",
    "\n",
    "#### For each cutout:\n",
    "- Scale all data between (0, 1) with:<br>\n",
    "$data\\_norm = \\frac{data - min\\_all}{max\\_all - min\\_all}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_3(data):\n",
    "    data_norm = (data - min_pixel_all) / (max_pixel_all - min_pixel_all)\n",
    "    return data_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technique 4: \n",
    "\n",
    "#### For each cutout:\n",
    "- Find the minimum pixel value in the image\n",
    "- Subtract that value off of each pixel\n",
    "- Divide each pixel in the image by the maximum value over ALL images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_4(data):\n",
    "    min_data = np.amin(data)\n",
    "    data_min_subtracted = data - min_data\n",
    "    data_norm = data_min_subtracted/max_pixel_all\n",
    "    return data_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technique 5:\n",
    "\n",
    "The same as technique 4, but now we take the log of each value first\n",
    "\n",
    "#### For each cutout:\n",
    "- Log each pixel (and the maximum pixel value)\n",
    "- Find the minimum pixel value in the image\n",
    "- Subtract that value off of each pixel\n",
    "- Divide each pixel in the image by the maximum value over ALL images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_5(data):\n",
    "    log_data = np.log10(data)\n",
    "    log_max_pixel = np.log10(max_pixel_all)\n",
    "    min_data = np.amin(log_data)\n",
    "    data_min_subtracted = log_data - min_data\n",
    "    data_norm = data_min_subtracted/log_max_pixel\n",
    "    return data_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying normalization techniques:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run each normalization technique on each cutout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "techniques = [norm_1, norm_2, norm_3, norm_4, norm_5]\n",
    "\n",
    "for num, technique in enumerate(techniques):\n",
    "    gal_star_norm = pd.DataFrame({})\n",
    "    for idx, row in tqdm(gal_star.iterrows(), total=gal_star.shape[0], desc='Technique '+str(num+1), leave=True):\n",
    "        # Separate type and data\n",
    "        raw_data = row[1:].values\n",
    "        obj_id = row[:1].values \n",
    "\n",
    "        # Run raw data through each normalization technique\n",
    "        norm_data = technique(raw_data)\n",
    "\n",
    "        # Append values for current cutout to dataframe\n",
    "        gal_star_norm = gal_star_norm.append([np.append(obj_id, norm_data)], ignore_index=True)\n",
    "\n",
    "    # Append plot data for first 5 images\n",
    "    plot_data = pd.concat([plot_data, pd.DataFrame({'norm_'+str(num+1): gal_star_norm[:5].to_numpy().flatten()})], axis=1)\n",
    "\n",
    "    # Save to .csv\n",
    "    gal_star_norm.to_csv('norm_'+str(num+1)+'_image_data.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot for first 5 objects to visualize what each normalization method is doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "technique_names = ['Raw Data', 'Technique 1', 'Technique 2', 'Technique 3', 'Technique 4', 'Technique 5']\n",
    "num_objects_to_plot = 5\n",
    "\n",
    "fig, axes = plt.subplots(nrows=num_objects_to_plot, ncols=6, sharex=True, sharey=True, constrained_layout=True, figsize=(18, 15))\n",
    "\n",
    "for technique in range(6):\n",
    "    data_technique = plot_data.iloc[:,technique]\n",
    "    new_data_technique = np.reshape(data_technique.values, (num_objects_to_plot, 401))\n",
    "    for idx, obj in enumerate(range(num_objects_to_plot)):\n",
    "        new_data = np.reshape(new_data_technique[idx][1:], (20, 20))\n",
    "        new_data = (new_data - np.amin(new_data)) / (np.amax(new_data) - np.amin(new_data))\n",
    "        im = axes[idx][technique].imshow(new_data, cmap='gray')\n",
    "        plt.colorbar(im, orientation=\"horizontal\", ax=axes)\n",
    "        if new_data_technique[idx][0] == 0.0:\n",
    "            name = 'Star'\n",
    "        elif new_data_technique[idx][0] == 1.0:\n",
    "            name = 'Galaxy'\n",
    "        if idx == 0:\n",
    "            axes[idx][technique].annotate(technique_names[technique], xy=(0.5, 1.1),\n",
    "                xycoords='axes fraction', size='xx-large', ha='center', va='baseline')\n",
    "        if technique == 0:\n",
    "            axes[idx][technique].annotate(name, xy=(-.2, 0.6), xycoords='axes fraction', \n",
    "                size='xx-large', ha='right', va='center')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
