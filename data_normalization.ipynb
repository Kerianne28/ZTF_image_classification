{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing image data in preparation for classification:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data can often be manipulated in such a way that the underlying information isn't altered, but the data is better prepared for input into machine learning algorithms. Such manipulation is referred to as \"pre-processing\", and usually involves scaling (or \"standardizing\") the data, or applying the same operation to each data point, such that the underlying data is not lost, but the data is now transformed into something that results in better performance for machine learning algorithms.\n",
    "\n",
    "### This notebook:\n",
    "- Opens each cutout\n",
    "- Applies 4 different data normalization techniques to each cutout\n",
    "- Flattens each image (in both raw and normalized forms), in preparation for the classifier\n",
    "- Saves all flatted data to .csv format in correct form for the classifier\n",
    "- Plots a few cutouts so you can visualize what the normalizing techniques are doing to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import astropy.io.fits as fits\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting list of all cutout paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutout_path = 'cutouts/'\n",
    "cutout_files = []\n",
    "gals = 0\n",
    "stars = 0\n",
    "\n",
    "for file in os.listdir(cutout_path):\n",
    "    if file.endswith('.fits'):\n",
    "        cutout_files.append(file)\n",
    "        if file.startswith('gal'):\n",
    "            gals += 1\n",
    "        elif file.startswith('star'):\n",
    "            stars += 1\n",
    "\n",
    "print(\"Found\", len(cutout_files), \"cutouts:\", stars, \"stars, and\", gals, \"gals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten and save the un-normalized \"raw\" image data for every cutout. \n",
    "\n",
    "- Open each cutout\n",
    "- Set truth labels (0=Star, 1=Galaxy)\n",
    "- Flatten the data (turns each 20x20 pixel image from shape (20, 20) to shape (400,))\n",
    "- Save to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gal_star = pd.DataFrame({})\n",
    "\n",
    "for idx, file in enumerate(tqdm(cutout_files)):\n",
    "    image = fits.getdata(cutout_path+str(file))\n",
    "    \n",
    "    if file.startswith('star')==True:\n",
    "        obj_id = 0\n",
    "    if file.startswith('galaxy')==True:\n",
    "        obj_id = 1\n",
    "         \n",
    "    data_flattened = image.flatten()\n",
    "    \n",
    "    gal_star = gal_star.append([np.append(obj_id, data_flattened)], ignore_index=True)\n",
    "    \n",
    "gal_star.to_csv('raw_image_data.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define each data normalization technique function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technique 1: \n",
    "\n",
    "#### For each cutout:\n",
    "- Take the log of each pixel value\n",
    "- Find the minimum pixel value accross the image\n",
    "- Subtract that minimum value from each pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_1(data):\n",
    "    data_log = np.log10(data)\n",
    "    min_log_data = np.amin(data_log)\n",
    "    data_norm = data_log - min_log_data\n",
    "    return data_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technique 2:\n",
    "\n",
    "#### For each cutout:\n",
    "- Calculate minimum pixel value across image\n",
    "- Calculate maximum pixel value accross image\n",
    "- Scale all data between (0, 1) with:<br>\n",
    "$data\\_norm = \\frac{data - min}{max - min}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_2(data):\n",
    "    min_data = np.min(data)\n",
    "    max_data = np.max(data)\n",
    "    data_norm = (data - min_data) / (max_data - min_data)\n",
    "    return data_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technique 3 & 4: \n",
    "\n",
    "This technique is split into 2 parts (because you need to pause in the middle to calculate the maximum pixel value over ALL images). The only difference between technique 3 & 4 is that you log the data first in technique 4. From start to end, this technique does the following:\n",
    "\n",
    "#### For each cutout:\n",
    "- Log each pixel (**technique 4 only**)\n",
    "- Find the minimum pixel value in the image\n",
    "- Subtract that value off of each pixel\n",
    "- Calculate the max value over the entire cutout\n",
    "- Once you caluclate the maximum value for each image, calculate the maximum of those (giving you the maximum pixel value over ALL images.\n",
    "- Divide each pixel in the image by the maximum value over ALL images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_3_4_part_1(data, log=bool):\n",
    "    if log:\n",
    "        data = np.log10(data)\n",
    "    min_data = np.amin(data)\n",
    "    data_min_subtracted = data - min_data\n",
    "    max_val = np.amax(data_min_subtracted)\n",
    "    return data_min_subtracted, max_val\n",
    "\n",
    "def norm_3_4_part_2(data, max_val):\n",
    "    data_norm = data/max_val\n",
    "    return data_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying normalization techniques:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining variables for appending to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty DataFrame (per technique) to append each normalized image to\n",
    "gal_star_norm_1 = pd.DataFrame({})\n",
    "gal_star_norm_2 = pd.DataFrame({})\n",
    "gal_star_norm_3 = pd.DataFrame({})\n",
    "gal_star_norm_4 = pd.DataFrame({})\n",
    "\n",
    "# Create empty list to append max pixel value from each cutout (technique 3 & 4 only)\n",
    "max_pixel_all_images_3 = []\n",
    "max_pixel_all_images_4 = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run each normalization technique on each cutout:\n",
    "\n",
    "***Note:*** Only part 1 of technique 3 & 4 happen here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx, row in tqdm(gal_star.iterrows(), total=gal_star.shape[0]):\n",
    "    # Separate type and data\n",
    "    raw_data = row[1:].values\n",
    "    obj_id = row[:1].values \n",
    "    \n",
    "    # Run raw data through each normalization technique\n",
    "    norm_1_data = norm_1(raw_data)\n",
    "    norm_2_data = norm_2(raw_data)\n",
    "    norm_3_data, max_val_3 = norm_3_4_part_1(raw_data, log=False)\n",
    "    norm_4_data, max_val_4 = norm_3_4_part_1(raw_data, log=True)\n",
    "    \n",
    "    # For technique 3 & 4: append the max pixel value for the current cutout\n",
    "    max_pixel_all_images_3.append(max_val_3)\n",
    "    max_pixel_all_images_4.append(max_val_4)\n",
    "    \n",
    "    # Append values for current cutout to corresponding dataframe\n",
    "    gal_star_norm_1 = gal_star_norm_1.append([np.append(obj_id, norm_1_data)], ignore_index=True)\n",
    "    gal_star_norm_2 = gal_star_norm_2.append([np.append(obj_id, norm_2_data)], ignore_index=True)\n",
    "    gal_star_norm_3 = gal_star_norm_3.append([np.append(obj_id, norm_3_data)], ignore_index=True)\n",
    "    gal_star_norm_4 = gal_star_norm_4.append([np.append(obj_id, norm_4_data)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save technique 1 & 2 to .csv for the classifier, as they are complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gal_star_norm_1.to_csv('norm_1_image_data.csv', header=False, index=False)\n",
    "gal_star_norm_2.to_csv('norm_2_image_data.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finish technique 3 & 4, and save to .csv for the classifier as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gal_star_norm_3_final = pd.DataFrame({})\n",
    "gal_star_norm_4_final = pd.DataFrame({})\n",
    "\n",
    "for idx, row in tqdm(gal_star_norm_3.iterrows(), total=gal_star_norm_3.shape[0], desc='Finishing technique 3'):\n",
    "    data = row[1:].values\n",
    "    obj_id = row[:1].values\n",
    "    norm_3_data_final = norm_3_4_part_2(data, np.amax(max_pixel_all_images_3))\n",
    "    gal_star_norm_3_final = gal_star_norm_3_final.append([np.append(obj_id, norm_3_data_final)], ignore_index=True)\n",
    "    \n",
    "for idx, row in tqdm(gal_star_norm_4.iterrows(), total=gal_star_norm_4.shape[0], desc='Finishing technique 4'):\n",
    "    data = row[1:].values\n",
    "    obj_id = row[:1].values\n",
    "    norm_4_data_final = norm_3_4_part_2(data, np.amax(max_pixel_all_images_4))\n",
    "    gal_star_norm_4_final = gal_star_norm_4_final.append([np.append(obj_id, norm_4_data_final)], ignore_index=True)\n",
    "    \n",
    "gal_star_norm_3_final.to_csv('norm_3_image_data.csv', header=False, index=False)\n",
    "gal_star_norm_4_final.to_csv('norm_4_image_data.csv', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot for first 5 object to visualize what each normalization method is doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(raw, norm1, norm2, norm3, norm4, obj_id):\n",
    "    if obj_id == 0.0:\n",
    "        object_name = 'Star:'\n",
    "    if obj_id == 1.0:\n",
    "        object_name = 'Galaxy:'\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=5, sharex=True, sharey=True, figsize=(15, 10))\n",
    "    \n",
    "    axes[0].imshow(raw, cmap='gray')\n",
    "    axes[1].imshow(norm1, cmap='gray')\n",
    "    axes[2].imshow(norm2, cmap='gray')\n",
    "    axes[3].imshow(norm3, cmap='gray')\n",
    "    axes[4].imshow(norm4, cmap='gray')\n",
    "    \n",
    "    axes[0].title.set_text(str(object_name)+' Raw Data')\n",
    "    axes[1].title.set_text(str(object_name)+' Norm 1')\n",
    "    axes[2].title.set_text(str(object_name)+' Norm 2')\n",
    "    axes[3].title.set_text(str(object_name)+' Norm 3')\n",
    "    axes[4].title.set_text(str(object_name)+' Norm 4')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(5):\n",
    "    obj_id = gal_star[0][idx]\n",
    "    raw_data = np.reshape(gal_star.loc[idx][1:].values, (20, 20))\n",
    "    norm_1 = np.reshape(gal_star_norm_1.loc[idx][1:].values, (20, 20))\n",
    "    norm_2 = np.reshape(gal_star_norm_2.loc[idx][1:].values, (20, 20))\n",
    "    norm_3 = np.reshape(gal_star_norm_3_final.loc[idx][1:].values, (20, 20))\n",
    "    norm_4 = np.reshape(gal_star_norm_4_final.loc[idx][1:].values, (20, 20))\n",
    "    plot_image(raw_data, norm_1, norm_2, norm_3, norm_4, obj_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
