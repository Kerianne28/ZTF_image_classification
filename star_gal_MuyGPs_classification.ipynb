{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Star Galaxy Classification\n",
    "\n",
    "### In this notebook we run the un-normalized and normalized datasets through the MuyGPyS classifier (a python classifying function that uses the MuyGPS  Gaussian process hyperparameter estimation method), and compare the resulting accuracies.\n",
    "\n",
    "**Note:** Must have run `data_normalization.ipynb` to continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from MuyGPyS.examples.classify import do_classify\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in all flattened data (normalized and un-normalized):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gal_star = pd.read_csv('raw_image_data.csv')\n",
    "gal_star_norm_1 = pd.read_csv('norm_1_image_data.csv')\n",
    "gal_star_norm_2 = pd.read_csv('norm_2_image_data.csv')\n",
    "gal_star_norm_3 = pd.read_csv('norm_3_image_data.csv')\n",
    "gal_star_norm_4 = pd.read_csv('norm_4_image_data.csv')\n",
    "gal_star_norm_5 = pd.read_csv('norm_5_image_data.csv')\n",
    "\n",
    "# Create a list with the name of the variable holding the data, \n",
    "# and the name you want associated with the data, for each dataset\n",
    "data_files = [[gal_star, 'Raw data'], \n",
    "              [gal_star_norm_1, 'Normalized data 1'], \n",
    "              [gal_star_norm_2, 'Normalized data 2'], \n",
    "              [gal_star_norm_3, 'Normalized data 3'],\n",
    "              [gal_star_norm_4, 'Normalized data 4'],\n",
    "              [gal_star_norm_5, 'Normalized data 5']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function that generates \"one-hot\" values.\n",
    "\n",
    "This essentially just takes our truth labels of 0 and 1, and does the following conversions for use in the classifier:\n",
    "- 0 to [1., -1.]\n",
    "- 1 to [-1., 1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_onehot_value(values):\n",
    "    onehot = []\n",
    "    for val in values:\n",
    "        if val == 0:\n",
    "            onehot.append([1., -1.])\n",
    "        elif val == 1:\n",
    "            onehot.append([-1., 1.])\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the classifier on each dataset\n",
    "\n",
    "For each dataset (un-normalized and normalized) in `data_files`, this for loop does the following:\n",
    "- Separate labels from data\n",
    "- Split up data between training and testing\n",
    "    - `test_size` is the fraction of the data you want to use for testing, where 0.5 means half of the data is used for testing and half for training.\n",
    "    - `random_state` makes each dataset get trained and tested on the same number of stars and galaxies.\n",
    "- Gets the one-hot values for the testing and training labels\n",
    "- Gets `train` and `test` into the proper format for the classifier, a dictionary with the keys: \n",
    "    - 'input': \n",
    "    - 'output':\n",
    "    - 'lookup':\n",
    "- Does the classification (`do_classify`)\n",
    "- Computes the accuracy of the classifier for the given dataset, by compairing predicted labels to truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, data_label in data_files:\n",
    "    truth_labels = data.iloc[:, 0].values\n",
    "    image_data = data.iloc[:, 1:].values\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(image_data, truth_labels, test_size=0.5, random_state=42)\n",
    "\n",
    "    print(\"=============== \", data_label, \" ===============\")\n",
    "    print('Training data:', len(y_train[y_train==0]), 'stars and', len(y_train[y_train==1]), 'galaxies')\n",
    "    print('Testing data:', len(y_test[y_test==0]), 'stars and', len(y_test[y_test==1]), 'galaxies')\n",
    "\n",
    "    onehot_train, onehot_test = generate_onehot_value(y_train), generate_onehot_value(y_test)\n",
    "\n",
    "    train = {'input': X_train, 'output': np.array(onehot_train), 'lookup': y_train}\n",
    "    test = {'input': X_test, 'output': np.array(onehot_test), 'lookup': y_test}\n",
    "\n",
    "    print(\"Running Classifier on\", data_label)\n",
    "    surrogate_predictions = do_classify(train, test, nn_count=30, embed_dim=30, verbose=False) #Switch verbose to True for more output\n",
    "    predicted_labels = np.argmax(surrogate_predictions, axis=1)\n",
    "    print(\"Total accuracy for\", data_label, \":\", np.around((np.sum(predicted_labels == np.argmax(test[\"output\"], axis=1))/len(predicted_labels))*100, 3), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>***Note:*** Each time you run the classifier will result in different accuracies.</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As you can see, all 5 normalization techniques do much better than the un-normalized data, with some performing better than others.\n",
    "\n",
    "### Things you can try, to see how they affect the classifier accuracy:\n",
    "- Play around with different values of `test_size`. What does testing on more or less data do?\n",
    "- Play around with different parameters that are passed to `do_classify`. Start with `nn_count` and `embed_dim`(For what those arguments are, and a full list of all of the arguments you can pass to do_classify, look at the function `do_classify` in `/MuyGPyS/examples/classify.py`).\n",
    "- Try generating more cutouts using `generating_ZTF_cutouts_from_ra_dec.ipynb`. How does having more testing and training data affects the classifier?\n",
    "- Play around with the parameters used to make the cutouts. What happens if you remove blend cuts? Can the classifier classify blends? What is you increase the seeing limit? Can the classifier classify images with bad atmoshperic quality?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <u>**Optional Step:**</u>\n",
    "### Running each dataset through the classifier multiple times, testing and training on varying amounts of data, different random states, and plotting the accuracy outcomes\n",
    "\n",
    "- Each time you run the following steps, you change:\n",
    "    - `test_size`: This is used in `train_test_split`, and changes the size of the testing and training datasets, which effects the accuracy of the classifier.\n",
    "    - `random_state`: This is used in `train_test_split`, and changes the ratio of how many stars-to-galaxies get tested on.\n",
    "- You can set how many times to run the classifier with varying test sizes and random states by setting `num_runs`, and you can manually change the test_size values by editing `test_size_values`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size_values = [.2, .25, .33, .4, .5, .75]\n",
    "num_runs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classifier(image_data, truth_labels, test_size, state):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(image_data, truth_labels, test_size=test_size, random_state=state)\n",
    "    onehot_train, onehot_test = generate_onehot_value(y_train), generate_onehot_value(y_test)\n",
    "    train = {'input': X_train, 'output': np.array(onehot_train), 'lookup': y_train}\n",
    "    test = {'input': X_test, 'output': np.array(onehot_test), 'lookup': y_test}\n",
    "    surrogate_predictions= do_classify(train, test, nn_count=30, embed_dim=30, verbose=False) #Switch verbose to True for more output\n",
    "    predicted_labels = np.argmax(surrogate_predictions, axis=1)\n",
    "    accuracy = (np.sum(predicted_labels == np.argmax(test[\"output\"], axis=1))/len(predicted_labels))*100\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "accuracies = pd.DataFrame({'test_size': test_size_values})\n",
    "\n",
    "# Setting progress bar for each time the classifier will be run during this step\n",
    "pbar = tqdm(total=len(data_files)*num_runs*len(test_size_values), desc='Running classifier', leave=True)\n",
    "\n",
    "for data, data_label in data_files:\n",
    "    truth_labels = data.iloc[:, 0].values\n",
    "    image_data = data.iloc[:, 1:].values\n",
    "    all_acc_dataset = []\n",
    "    for test_size in test_size_values:\n",
    "        acc = []\n",
    "        idx = 1\n",
    "        while idx <= num_runs:\n",
    "            accuracy = run_classifier(image_data, truth_labels, test_size, state=random.randint(0, 10000))\n",
    "            acc.append(accuracy)\n",
    "            pbar.update(1)\n",
    "            idx += 1\n",
    "        avg_acc = np.average(acc)\n",
    "        all_acc_dataset.append(avg_acc)\n",
    "    temp_df = pd.DataFrame({str(data_label): all_acc_dataset})\n",
    "    accuracies = pd.concat([accuracies, temp_df], axis=1)\n",
    "display(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "for data, data_labels in data_files:\n",
    "    plt.plot(accuracies['test_size'].values, accuracies[data_labels].values, label=data_labels)\n",
    "    \n",
    "plt.legend(fontsize=12)   \n",
    "plt.tick_params(labelsize=14)\n",
    "plt.xlabel(\"Test size (as a ratio to full data size)\", fontsize=18)\n",
    "plt.ylabel(\"Accuracy [%]\", fontsize=18)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
