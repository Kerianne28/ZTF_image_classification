{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Star Galaxy Classification\n",
    "\n",
    "### In this notebook we run the MuyGPyS classifier on the un-normalized and normalized image data, and compares the accuracy of each.\n",
    "\n",
    "***Note:*** Must have run 'data_normalization.ipynb' to continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from MuyGPyS.examples.classify import do_classify\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in all flattened data (normalized and un-normalized):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gal_star = pd.read_csv('raw_image_data.csv')\n",
    "gal_star_norm_1 = pd.read_csv('norm_1_image_data.csv')\n",
    "gal_star_norm_2 = pd.read_csv('norm_2_image_data.csv')\n",
    "gal_star_norm_3 = pd.read_csv('norm_3_image_data.csv')\n",
    "gal_star_norm_4 = pd.read_csv('norm_4_image_data.csv')\n",
    "\n",
    "# Create a list with the name of the variable holding the data, \n",
    "# and the name you want associated with the data, for each dataset\n",
    "data_files = [[gal_star, 'Raw data'], \n",
    "              [gal_star_norm_1, 'Normalized data 1'], \n",
    "              [gal_star_norm_2, 'Normalized data 2'], \n",
    "              [gal_star_norm_3, 'Normalized data 3'], \n",
    "              [gal_star_norm_4, 'Normalized data 4']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function that generates \"one-hot\" values.\n",
    "\n",
    "This essentially just takes our truth labels of 0 and 1, and does the following conversions for use in the classifier:\n",
    "- 0 to [1., -1.]\n",
    "- 1 to [-1., 1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_onehot_value(values):\n",
    "    onehot = []\n",
    "    for val in values:\n",
    "        if val == 0:\n",
    "            onehot.append([1., -1.])\n",
    "        elif val == 1:\n",
    "            onehot.append([-1., 1.])\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the classifier on each dataset\n",
    "\n",
    "For each dataset (un-normalized and normalized) in `data_files`, this for loop does the following:\n",
    "- Separate labels from data\n",
    "- Split up data between training and testing\n",
    "    - `test_size` is the fraction of the data you want to use for testing, where 0.5 means half of the data is used for testing and half for training.\n",
    "    - `random_state` makes each dataset get trained and test on the same number of stars and galaxies, for better comparison of the algorithms. Feel free to change the random_state value, or get rid of it all together to change it up!\n",
    "- Gets the one-hot values for the testing and training labels\n",
    "- Defines `train` and `test` into the proper form for the classifier, a dictionary with the keys: \n",
    "    - 'input': \n",
    "    - 'output':\n",
    "    - 'lookup':\n",
    "- Does the classification (`do_classify`)\n",
    "- Computes the accuracy of the classifier for the given dataset, by compairing to truth labels.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for data, data_label in data_files:\n",
    "    truth_labels = data.iloc[:, 0].values\n",
    "    image_data = data.iloc[:, 1:].values\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(image_data, truth_labels, test_size=0.5, random_state=28)\n",
    "    \n",
    "    print(\"=============== \", data_label, \" ===============\")\n",
    "    print('Training data:', len(y_train[y_train==0]), 'stars and', len(y_train[y_train==1]), 'galaxies')\n",
    "    print('Testing data:', len(y_test[y_test==0]), 'stars and', len(y_test[y_test==1]), 'galaxies')\n",
    "    \n",
    "    onehot_train, onehot_test = generate_onehot_value(y_train), generate_onehot_value(y_test)\n",
    "    \n",
    "    train = {'input': X_train, 'output': np.array(onehot_train), 'lookup': y_train}\n",
    "    test = {'input': X_test, 'output': np.array(onehot_test), 'lookup': y_test}\n",
    "    \n",
    "    print(\"Running Classifier on\", data_label)\n",
    "    surrogate_predictions= do_classify(train, test, nn_count=50, verbose=False) #Switch verbose to True for more output\n",
    "    \n",
    "    predicted_labels = np.argmax(surrogate_predictions, axis=1)\n",
    "    \n",
    "    print(\"Total accuracy for\", data_label, \":\", np.around((np.sum(predicted_labels == np.argmax(test[\"output\"], axis=1))/len(predicted_labels))*100, 3), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>***Note:*** Each time you run the classifier will result in different accuracies.</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As you can see, all 4 normalization techniques do much better than the un-normalized data, with technique 3 & 4 performing the best in most cases.\n",
    "\n",
    "### Things you can try, to see how they affect the classifier accuracy:\n",
    "- Play around with different values of `test_size`. What does testing on more or less data do?\n",
    "- Try generating more cutouts using `generating_ZTF_cutouts_from_ra_dec.ipynb`. How does having more testing and training data affects the classifier?\n",
    "- Play around with the parameters used to make the cutouts. What happens if you remove blend cuts? Can the classifier classify blends? What is you increase the seeing limit? Can the classifier classify images with bad atmoshperic quality?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
